# -*- coding: utf-8 -*-
"""exam13_classfication_fashion_mnist_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ij8B5K7JpbLUi7hPcemiUINNAi0ec9z3
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Activation
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout
from tensorflow.keras.optimizers import Adam
from keras.utils import to_categorical  #onehotencoder해주는 애임.
from tensorflow.keras import datasets

"""Convolutional Neural Network (CNN 또는 ConvNet)은 주로 이미지 처리에 사용되는 딥 러닝 모델입니다. CNN은 입력 데이터에서 특징을 자동으로 학습하고, 이러한 특징을 기반으로 패턴을 인식하고 분류하는 데 효과적입니다. 아래에서 CNN의 주요 구성 요소와 작동 원리를 자세히 설명하겠습니다.

1. 합성곱층 (Convolutional Layer):
CNN의 핵심 부분으로, 이미지에서 특징을 추출하는 역할을 합니다.
각 합성곱층은 여러 개의 필터(커널)를 사용하며, 이 필터는 이미지를 훑으면서 특정한 패턴이나 특징을 찾습니다.
합성곱 연산은 필터를 이미지에 적용하고, 각 위치에서 필터의 가중치를 곱해서 총합을 계산하는 것으로 이루어집니다.
활성화 함수(주로 ReLU)를 통과한 결과가 출력되며, 이를 특징 맵(feature map)이라고 합니다.
MaxPooling 등의 다양한 풀링 연산은 특징 맵을 다운샘플링하여 계산량을 줄이고 중요한 정보를 추출합니다.

2. 피처 맵 (Feature Map):
각 합성곱층에서 나오는 출력을 피처 맵이라고 부릅니다.
피처 맵은 입력 이미지에서 감지된 특징을 나타내며, 점차적으로 더 추상적인 특징을 학습하게 됩니다.
3. 완전 연결층 (Fully Connected Layer):
피처 맵을 Flatten 작업을 통해 1차원 벡터로 펼친 후, 완전 연결층에 연결합니다.
완전 연결층은 이미지의 전역적인 특징을 학습하고, 최종적으로 분류를 수행합니다.
4. 파라미터 공유 (Parameter Sharing):
합성곱층에서 사용되는 필터는 이미지 전체에 대해 공유됩니다.
이로써 모델은 이미지의 어떤 위치에서든 특정 패턴을 감지할 수 있게 됩니다.
파라미터 공유는 모델의 학습 파라미터 수를 크게 감소시키고, 지역적인 패턴을 인식하는 데 효과적입니다.
5. 전이 학습 (Transfer Learning):
사전 훈련된 CNN 모델을 가져와 새로운 작업에 활용하는 전이 학습이 흔히 사용됩니다.
대규모 이미지 데이터셋에서 훈련된 모델은 일반적인 특징을 학습하였기 때문에, 적은 양의 데이터로도 높은 성능을 달성할 수 있습니다.
CNN은 이미지 분류 뿐만 아니라 객체 감지, 분할 등 다양한 컴퓨터 비전 작업에서 효과적으로 사용되며, 최근에는 음성 처리, 자연어 처리 등 다양한 분야에서도 활용되고 있습니다.
"""

(X_train,Y_train), (X_test,Y_test) = datasets.fashion_mnist.load_data()
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

label = ['T-shirt', 'trouser', 'pullover', 'dress', 'coat','sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']

my_sample = np.random.randint(60000)
plt.imshow(X_train[my_sample], cmap='gray')
plt.show()
print(Y_train[my_sample])
print(X_train[my_sample])

y_train = to_categorical(Y_train)
y_test = to_categorical(Y_test)
print(Y_train[5000])
print(y_train[5000])

x_train = X_train / 255
x_test = X_test / 255
x_train = x_train.reshape(60000,28,28,1)    #애초에 2차원이라서 reshape 안해도 되지만 채널수 등록하려고 이렇게 reshape함.
x_test = x_test.reshape(10000,28,28,1)
print(x_train.shape)

"""Reshape (형태 변경):

Convolutional Neural Network (CNN) 모델은 2D 이미지를 처리하는데 적합하도록 설계되었습니다. reshape를 사용하여 데이터를 28x28 크기의 2D 이미지로 변환합니다. 모델에 입력으로 들어가는 이미지의 형태를 (데이터 수, 높이, 너비, 채널 수)로 만들어야 합니다. 여기서는 흑백 이미지이므로 채널 수가 1입니다.

cnn은 2차원에서 이미지를 다룸 처음에 데이터 받아온게 2차원형태라서 reshape 따로 안써도 됨 근데 쓴건 채널수인 흑백 인지 rgb인지 알려주려고 사용함 1은 흑백 rgb는 3 즉 흑백은 색깔1개로 봐도 되니까 채널1개 rgb은 색깔 3개니까 채널3개임.

reshape 함수에서 -1은 해당 차원의 크기를 자동으로 계산하라는 의미를 갖습니다.

여기서는 x_train과 x_test를 2차원 배열로 변환하고 있습니다. 각 이미지는 28x28 크기이므로, reshape(-1, 28*28)은 이미지 데이터를 2차원 배열로 펼치되 각각의 행이 28x28의 이미지를 나타내도록 합니다.

이때 -1을 사용하면, 해당 차원의 크기를 다른 차원들과 호환되도록 자동으로 계산됩니다. 즉, 전체 데이터 크기를 유지하면서 나머지 차원을 맞춰줍니다.

따라서, x_train과 x_test의 shape는 (데이터의 개수, 28*28)로 변환되며, -1을 사용함으로써 데이터의 개수에 따라 자동으로 조절되게 됩니다. 최종적으로 x_train과 x_test는 각각 이미지 데이터를 펼친 2차원 배열이 됩니다.

reshape는 배열이나 텐서의 형태를 변경하는 작업을 말합니다. 이 작업은 데이터를 특정 형태로 바꿔야 할 때 사용되며, 주로 딥러닝에서 데이터 전처리나 모델의 입력 형태를 맞추기 위해 사용됩니다. 여러 이유와 장점이 있습니다.

이유와 장점:

모델의 입력 형태 맞춤:

딥러닝 모델은 특정한 입력 형태를 요구합니다. 예를 들어, 이미지를 처리하는 모델은 3D 텐서 형태의 입력을 기대할 수 있습니다. 따라서, 데이터를 모델에 맞게 변환하기 위해 reshape를 사용할 수 있습니다.
다양한 데이터 형태에 대응:

reshape를 사용하면 다양한 데이터 형태에 대응할 수 있습니다. 예를 들어, 이미지 데이터를 1D 벡터로 평탄화하거나, 1D 시계열 데이터를 2D 형태로 변환하는 등 다양한 형태로 데이터를 조정할 수 있습니다.
데이터 전처리:

reshape는 데이터를 전처리하는 데 유용합니다. 예를 들어, 이미지 데이터를 모델에 입력하기 전에 정규화를 수행하거나, 특정한 형태로 바꾸어 데이터의 특성을 강조할 수 있습니다.
다양한 라이브러리와 호환성:

다양한 딥러닝 라이브러리나 프레임워크에서는 특정한 형태의 입력을 기대하므로, 데이터를 해당 형태로 맞추기 위해 reshape를 사용합니다.
모델 학습 성능 향상:

모델의 학습 성능을 향상시키기 위해 데이터를 특정한 형태로 변환할 수 있습니다. 예를 들어, 이미지 데이터를 평균이 0이고 표준편차가 1인 형태로 정규화할 수 있습니다.
시각화 및 디버깅:

데이터의 형태를 적절하게 조정하면 시각화나 디버깅이 훨씬 용이해집니다. 예를 들어, 이미지 데이터를 시각화할 때 3D 텐서로 형태를 맞추면 이미지를 시각화하기 쉽습니다.
언제 사용해야 하는지:

모델의 입력 형태를 맞출 때: 모델에 입력으로 전달되는 데이터의 형태가 모델의 기대 형태와 일치하지 않을 때 사용합니다.

데이터 전처리 시: 데이터를 모델에 입력하기 전에 정규화, 평탄화 등의 전처리 작업을 할 때 사용합니다.

시각화나 디버깅을 위해: 데이터의 형태를 시각화하거나 디버깅하기 쉬운 형태로 조정할 때 사용합니다.

datasets.mnist.load_data()을 통해 로드된 MNIST 데이터셋은 손으로 쓴 숫자(0부터 9까지)의 이미지로 구성되어 있습니다. 각 이미지는 28x28 픽셀의 흑백 이미지로 표현됩니다. 일반적으로 딥러닝 모델에 이미지를 입력으로 사용할 때는 이미지의 형태를 모델이 처리할 수 있는 형태로 변환해야 합니다.

따라서, reshape를 사용하여 MNIST 데이터셋을 모델에 입력으로 사용하기 적합한 형태로 조정하는 이유는 다음과 같습니다:

모델의 기대 형태와 맞춤:

대부분의 딥러닝 모델은 2D 형태의 이미지를 처리합니다. MNIST 데이터셋의 이미지는 28x28 크기의 2D 배열이기 때문에, 모델에 입력으로 사용하려면 2D 형태로 변환해야 합니다.

채널 정보 추가:

Convolutional Neural Network (CNN)과 같은 모델은 일반적으로 이미지에 대한 채널 정보를 기대합니다. 하지만 MNIST는 흑백 이미지이므로 채널이 1개입니다. 따라서, reshape를 통해 데이터에 채널 차원을 추가하여 모델이 이를 올바르게 이해할 수 있도록 합니다.

datasets.fashion_mnist.load_data()를 통해 로드된 Fashion MNIST 데이터셋은 MNIST와 유사하지만, 각 이미지는 28x28 크기의 흑백 이미지로 이루어져 있습니다. 따라서, 이미지의 형태가 MNIST와 동일하게 28x28이며, 채널도 1개인 형태를 가지고 있습니다.

Fashion MNIST 데이터셋을 MNIST와 유사하게 reshape하지 않는 이유는:

이미지 형태가 동일하다:

Fashion MNIST와 MNIST는 이미지의 크기와 채널 수가 동일하므로, 모델에 입력으로 사용할 때 추가적인 reshape가 필요하지 않습니다.
채널 정보가 이미 존재한다:

Fashion MNIST는 흑백 이미지이며 채널이 1개이기 때문에, 이미 채널 정보가 포함되어 있습니다. 따라서 추가적인 reshape 작업 없이도 모델이 이를 올바르게 처리할 수 있습니다.

딥러닝 모델에서는 입력 데이터의 형태를 모델이 이해할 수 있는 형태로 맞추어야 합니다. 따라서, 모델의 입력층에 들어가는 데이터의 형태를 모델의 구조에 맞춰야 합니다. Convolutional Neural Network (CNN)과 같은 이미지 처리 모델에서는 이미지 데이터를 특정한 형태로 입력으로 받습니다.

Fashion MNIST 데이터셋의 이미지는 28x28 크기의 흑백 이미지이기 때문에, 일반적으로 CNN과 같은 모델에 사용하기 위해서는 reshape 작업이 필요합니다. 모델이 이미지의 크기와 채널 수를 인식하도록 하기 위함입니다. Conv2D 레이어 등의 모델은 2D 이미지를 입력으로 받기 때문에, 28x28 크기의 이미지를 (28, 28, 1) 형태로 바꾸어야 합니다.

따라서, 데이터를 reshape하지 않고 모델에 넣으면 모델은 입력 형태를 잘못 이해할 수 있어서 학습이 올바르게 이루어지지 않을 가능성이 있습니다. 따라서 일반적으로는 모델의 기대 입력 형태에 맞게 데이터를 reshape하여 사용하는 것이 좋습니다.
"""

model = Sequential()
model.add(Conv2D(32, input_shape=(28,28,1), activation='relu', kernel_size=(3, 3), padding='same')) #필터를 32개 넣어서 저 커널사이즈는 필터의 크기를 말함 아래사진처럼 3*3필터임
model.add(MaxPool2D(padding='same', pool_size=(2,2)))
model.add(Conv2D(32, activation='relu', kernel_size=(3, 3), padding='same'))
model.add(MaxPool2D(padding='same', pool_size=(2,2)))
model.add(Flatten())
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))
model.summary()

"""https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53

Layer (type): 현재 레이어의 종류와 이름입니다. 여기서는 Conv2D 레이어입니다.

Output Shape: 현재 레이어의 출력 형태입니다. (None, 28, 28, 32)는 네 개의 차원을 가진 텐서를 의미합니다. None은 배치 크기(batch size)를 나타냅니다.

Param #: 현재 레이어의 파라미터 수를 나타냅니다.

--------------

파라미터 수 계산:

파라미터는 가중치(weight)와 편향(bias)로 구성됩니다. Conv2D 레이어의 경우, 필터 개수가 32이고, 각 필터의 크기가 3x3입니다. 입력 채널의 수는 1이라고 가정하겠습니다(흑백 이미지를 처리하므로). 이러한 경우 파라미터 수는 다음과 같이 계산됩니다:

--------

가중치 파라미터 수: 32 (필터 개수) * 3 * 3 (필터 크기) * 1 (입력 채널 수) = 288

편향 파라미터 수: 32 (필터 개수) = 32

따라서 총 파라미터 수는 288 (가중치) + 32 (편향) = 320입니다.

-------

종합하면, conv2d_4 (Conv2D) (None, 28, 28, 32) 레이어는 28x28 크기의 입력을 받아 3x3 크기의 32개 필터로 합성곱을 수행하고, 그 결과로 28x28 크기의 32개 채널을 출력합니다. 이때 사용되는 파라미터 수는 320입니다.

이해를 돕기 위해 간단하게 설명하겠습니다.

채널(Channel):

채널은 이미지의 특정한 특징을 나타내는 부분입니다. 흔히 컬러 이미지의 경우, 각각의 RGB (Red, Green, Blue)가 하나의 채널을 형성합니다. 예를 들어, RGB 이미지는 세 개의 채널로 이루어져 있습니다.

그러나 흑백 이미지의 경우에도 하나의 채널로 간주됩니다. 각 픽셀은 단일 값을 가지며, 이 값은 픽셀의 밝기를 나타냅니다.

딥러닝에서는 합성곱 연산을 통해 이미지의 다양한 특징을 추출하기 위해 여러 채널을 사용합니다.

---------------------

필터(Filter):

필터는 입력 데이터에서 특정한 특징을 감지하는 데 사용되는 작은 행렬 또는 윈도우입니다.

Conv2D 레이어에서는 이 필터가 이미지에 적용되어 다양한 특징이나 패턴을 찾게 됩니다. 각 필터는 특정 방향, 엣지, 또는 색상 등을 감지할 수 있습니다.

예를 들어, 얼굴 인식 모델에서는 각각의 필터가 눈, 코, 입과 같은 특징을 감지하는 데 사용됩니다.

따라서, 채널은 이미지의 각 특징을 표현하고, 필터는 입력 데이터에서 이러한 특징을 찾아내는 데 사용됩니다. Conv2D 레이어에서는 여러 필터가 여러 채널에 동시에 적용되어 다양한 특징을 추출합니다.

필터를 같은 자리에 있는애들끼리 곱해서 1의 개수를 세면 한 픽셀이 됨. 아래 사진에서 오른쪽이 위에 있는게 필터이고 젤 오른쪽과 같이 하나하나 만들어지는거임.
즉 젤 왼쪽의 이미지에 가운데 필터를 넣어주면 젤 왼쪽의 이미지가 만들어짐.
가운데 위치한 필터와 같은게 32개가 만들어 질거임.

이런 필터의 픽셀값을 학습해서 이미지에 따라 적절한 필터가 만들어지게 됨.
그렇게해서 이미지를 판단할 수 있게 되는거임.

필터(Filter), 바이어스(Bias), 파라미터(Parameters), 입력 채널 수, 출력 채널 수는 컨볼루션 레이어에서 사용되는 중요한 개념들입니다. 간단한 예시를 통해 이들의 상관관계를 설명하고자 합니다.

예시: 컬러 이미지 필터링
1. 필터(Filter):

필터는 이미지에서 특정한 패턴이나 특징을 감지하는 작은 윈도우로 생각할 수 있습니다.
예를 들어, 에지를 감지하는 필터, 색상을 강조하는 필터 등이 있습니다.
2. 바이어스(Bias):

바이어스는 각 출력 채널에 추가되어 주는 상수 값입니다.
바이어스는 각 채널이 특정한 특징에 얼마나 민감한지를 조절하는 역할을 합니다.
3. 파라미터(Parameters):

파라미터는 가중치(필터의 가중치)와 바이어스의 합으로 이루어집니다.
가중치는 필터에 적용되는 값들을 나타내고, 바이어스는 각 출력 채널에 더해지는 상수 값입니다.
4. 입력 채널 수:

입력 채널 수는 컨볼루션 레이어에 입력되는 데이터의 채널 수를 나타냅니다.
컬러 이미지의 경우, 일반적으로 RGB 채널이므로 입력 채널 수는 3입니다.
흑백 이미지의 경우, 채널이 1개이므로 입력 채널 수는 1입니다.
5. 출력 채널 수:

출력 채널 수는 컨볼루션 레이어에서 생성되는 특성 맵의 채널 수를 나타냅니다.
각 출력 채널은 입력 데이터의 특정한 특징을 감지하는 역할을 합니다.

--------------


실생활 예시:
가상의 농장에서 각각 다른 종류의 작물을 인식하는 컨볼루션 네트워크를 생각해봅시다.

1. 필터(Filter):

필터는 각각 다른 종류의 작물을 인식하기 위한 도구로 비유할 수 있습니다. 각 필터는 특정 작물에 반응합니다.
2. 바이어스(Bias):

바이어스는 작물을 인식할 때 각 필터가 얼마나 민감한지를 조절하는 상수 값으로 비유할 수 있습니다. 예를 들어, 특정 작물이 다른 작물보다 어두워서 더 많은 빛이 필요하다면 해당 필터에 바이어스를 높일 수 있습니다.
3. 파라미터(Parameters):

각 필터에는 가중치(필터의 가중치)와 바이어스의 합으로 이루어진 파라미터가 있습니다. 이것들은 각 필터가 특정 작물을 얼마나 강하게 인식하는지를 결정합니다.
4. 입력 채널 수:

입력 데이터는 농장에서 촬영한 이미지이며, 컬러 이미지로 구성되어 있습니다. 따라서 입력 채널 수는 RGB 채널이므로 3입니다.
5. 출력 채널 수:

출력 채널 수는 농장에 존재하는 다양한 작물의 수와 동일합니다. 각 출력 채널은 특정 작물을 감지하는 역할을 합니다.

이와 같이 컨볼루션 네트워크는 필터, 바이어스, 파라미터, 입력 채널 수, 출력 채널 수 등의 요소를 조합하여 입력 이미지에서 다양한 특징을 추출하고, 특정 작물을 인식하는 데에 활용될 수 있습니다.


28* 28 짜리의 이미지 한개가 3* 3 필터 32개를 통과하면 28* 28 이미지가 32개로 늘어남.

28* 28 짜리의 이미지가 3* 3 필터를 통과하면 26* 26으로 줄어들게됨.
크기를 유지하기 위해 바깥쪽을 늘려서 0으로 채워주는데 이게 padding임.
padding을 same으로 주면 입력과 출력을 같게 해줌 즉 둘다 28*28되게 끔 padding함.
그래서 30*30이되서 3* 3 필터통과후 28* 28가 됨.

maxpool은 필터 씌우는거랑 비슷한데 위에 사이즈를 2* 2로 했으니 2* 2 필터로 생각해서 집은 곳 중에 가장큰값을 선택함. 근데 con2v와 다른점은 con2v는 한칸씩 진행되서 중복이 되었는데 maxpool은 중복되지않고 각각 진행해서 2* 2의 경우 크기가 절반으로 줄거임.
그래서 14* 14크기의 이미지가 완성될 거고 총 32개가 될거임.
padding을 same으로 주면 맨끝에 부족한 만큼 채워줘서 maxpool 필터를 적용할 수 있게 끔 해줌. 아래 사진이 maxpool하는거임.

그후에 다시똑같은 과정 반복해 보면 일단 14* 14짜리 이미지가 다시 3* 3짜리 32개가 32개 있으니 즉 14* 14짜리 이미지가 3* 3짜리가 32개가 32세트 만큼의 필터를 통과해서
14 *14짜리 크기가 32만큼의 개수가 생기게 됨. 즉 3* 3짜리가 32개로 14* 14를 이미지 한개를 필터를 씌우고 그런걸 32개의 세트로 한니 14 *14가 32개가 생기는거임. 그후 다시 2* 2를 지나면 7* 7이 됨.

Flatten이거는 reshape 해주는애라고 생각하면 됨.

그래서 7* 7 짜리 32개를 연산하면 7* 7* 32= 1568 이 되는데 1568개의 입력이되어서 128개의 뉴런과 연결되고 그 후에 쭈우욱 진행됨.

젤아래는 두번째 conv2D이거랑 maxpool 진행한거임.



첫 번째 Conv2D 레이어:

 32개의 3x3 크기 필터를 사용하여 28x28 크기의 입력 이미지에 합성곱을 수행합니다.
활성화 함수로 ReLU를 사용하며, 'padding'을 'same'으로 설정하여 출력 크기를 입력과 동일하게 유지합니다.
따라서, 출력은 여전히 28x28이지만, 각 필터에 의해 만들어진 특징들이 적용됩니다.

----------

첫 번째 MaxPooling2D 레이어:

 2x2 크기의 맥스 풀링을 수행하여 공간적인 차원을 절반으로 줄입니다.
'padding'을 'same'으로 설정하여 출력 크기를 입력과 동일하게 유지합니다.
출력은 14x14 크기의 32개 채널을 갖는 이미지입니다.

----------

두 번째 Conv2D 레이어:

 32개의 3x3 크기 필터를 사용하여 이전 단계의 출력에 다시 합성곱을 수행합니다.
활성화 함수는 ReLU이고, 'padding'을 'same'으로 설정하여 출력 크기를 유지합니다.
출력은 여전히 14x14 크기의 32개 채널을 갖는 이미지입니다.

----------

두 번째 MaxPooling2D 레이어:

 다시 2x2 크기의 맥스 풀링을 수행하여 출력 크기를 7x7로 줄입니다.
'padding'을 'same'으로 설정하여 출력 크기를 입력과 동일하게 유지합니다.
최종 출력은 7x7 크기의 32개 채널을 갖는 이미지입니다.

----------

Flatten 레이어:

 7x7x32의 3D 출력을 1D 벡터로 평탄화합니다.

----------

Dropout 레이어:

 20%의 뉴런을 무작위로 비활성화하여 과적합을 방지합니다.

----------

첫 번째 Dense (Fully Connected) 레이어:

 128개의 뉴런과 ReLU 활성화 함수를 갖는 완전 연결층입니다.

----------

두 번째 Dropout 레이어:

 다시 20%의 뉴런을 무작위로 비활성화하여 과적합을 방지합니다.

----------

두 번째 Dense (Fully Connected) 레이어:

 10개의 뉴런과 softmax 활성화 함수를 갖는 출력층입니다. 이 모델은 10개의 클래스에 대한 확률을 출력하게 됩니다.
"""

opt = Adam(learning_rate=0.01)
model.compile(opt, loss='categorical_crossentropy', metrics='accuracy')
fit_hist = model.fit(x_train, y_train, batch_size=256, epochs=5, validation_split=0.2, verbose=1)

score = model.evaluate(x_test, y_test, verbose=0)
print('accuracy',score[1])

plt.plot(fit_hist.history['accuracy'])
plt.show()

my_sample = np.random.randint(10000)
plt.imshow(X_test[my_sample], cmap='gray')
print(label[Y_test[my_sample]])
pred = model.predict(x_test[my_sample].reshape(-1, 28, 28, 1))
print(pred)
print(label[np.argmax(pred)])

