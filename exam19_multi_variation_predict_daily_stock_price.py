# -*- coding: utf-8 -*-
"""exam19_multi_variation_predict_daily_stock_price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6u1boqLzDLLEYRBvT9DJ7VQqA8umRHT

앞에서는 일부 데이터만을 가지고 예측했는데 이번에는 전체 데이터 다 활용할 거임.

종가(Close)"는 주식 시장에서 하루 동안 거래된 주식의 가격 중 마지막 거래 가격을 나타냅니다. 일반적으로 주식 차트에서는 시가(Open), 고가(High), 저가(Low), 종가(Close) 등의 정보가 포함되어 있습니다.

시가(Open): 해당 기간 동안의 첫 번째 거래 가격.
고가(High): 해당 기간 동안의 최고 거래 가격.
저가(Low): 해당 기간 동안의 최저 거래 가격.
종가(Close): 해당 기간 동안의 마지막 거래 가격.

앞에서는 종가만 가지고 처리했는데 이번에는 모두 활용할거임.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

raw_data = pd.read_csv('./datasets/samsung.csv')
print(raw_data.head())
raw_data.info()

raw_data['Date'] = pd.to_datetime(raw_data['Date']) #object타입에서 datatime64타입으로 바꿔줌 즉 시간타입으로 바꿔줌
raw_data.info()
print(raw_data.head())

raw_data.set_index('Date', inplace=True)    #Date를 인덱스로 만들어줌
raw_data.info()
print(raw_data.head())

data = raw_data['2021-01-03':'2024-01-22']
print(data.tail())
data.info()
print(data.shape)

minmax_scaler = MinMaxScaler()
scaled_data = minmax_scaler.fit_transform(data)
print(scaled_data[:5])
print(scaled_data.shape)

"""minmax_scaler.fit_transform(data)는 주어진 데이터프레임 data를 최소-최대 스케일링하여 정규화합니다

최소-최대 스케일링은 데이터의 값을 특정 범위로 변환하는 방법 중 하나입니다. 여러 데이터가 갖는 값의 범위가 다를 때, 이를 비슷한 범위로 조절해주는 역할을 합니다.

공식:

각 데이터를 최소값에서 뺀 후, 그 값을 최대값에서 최소값을 뺀 값으로 나눕니다.
이렇게 하면 모든 데이터가 0과 1 사이의 값으로 변합니다.

작동 방식:

예를 들어, 어떤 특성(예: 주식 가격)이 100부터 1000까지의 범위를 가진다고 가정합니다.
최소값이 100, 최대값이 1000이라면, 각 데이터에서 100을 빼고, 900으로 나누어주면 해당 특성의 모든 데이터는 0과 1 사이의 값으로 스케일링됩니다.

활용:

주로 머신러닝에서 모델을 학습할 때, 입력 데이터의 크기를 조절하는 데 사용됩니다.
특히, 신경망과 같은 알고리즘에서는 입력 데이터의 스케일이 모델 학습에 영향을 미칠 수 있어 중요합니다.

예시:

주식 가격 데이터를 예로 들면, 하루의 종가가 1000원에서 50000원까지 값이 다를 수 있습니다. 이를 최소-최대 스케일링하면 모든 종가가 0과 1 사이의 값으로 조절됩니다.
Scikit-learn을 활용:

파이썬의 Scikit-learn 라이브러리에서 MinMaxScaler를 사용하면 쉽게 최소-최대 스케일링을 할 수 있습니다. 코드는 간단하게 작성할 수 있습니다.


간단히 말하면, 최소-최대 스케일링은 데이터의 크기를 비슷한 범위로 조절하여 모델이 더 잘 학습할 수 있게 돕는 방법입니다.
"""

sequence_x = []
sequence_y = []
for i in range(len(scaled_data)-30):
    x = scaled_data[i:i+30]
    y = scaled_data[i+30]
    sequence_x.append(x)
    sequence_y.append(y)

sequence_x = np.array(sequence_x)
sequence_y = np.array(sequence_y)
print(sequence_x[0])
print(sequence_y[0])
print(sequence_x.shape)
print(sequence_y.shape)

"""여기까지가 데이터 전처리 임."""

x_train, x_test, y_train, y_test = train_test_split(sequence_x,sequence_y,test_size=0.2)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

model = Sequential()
model.add(GRU(50, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True, activation='tanh')) #x_train.shape[1]=30, [2]는 6 위에서 나온 출력값
model.add(GRU(50, activation='tanh'))
model.add(Flatten())
model.add(Dense(1))
model.summary()

"""1. Sequential 모델 생성:

model = Sequential()을 사용하여 Sequential 모델을 생성합니다.

2. 첫 번째 GRU 레이어:

model.add(GRU(50, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True, activation='tanh'))
첫 번째 GRU 레이어를 추가합니다.
50개의 뉴런을 가지며, 입력 형태는 (x_train.shape[1], x_train.shape[2])입니다. 여기서 x_train.shape[1]은 시퀀스 길이(예: 30), x_train.shape[2]는 각 시퀀스에서의 특성 수입니다.
return_sequences=True는 GRU 레이어의 출력을 시퀀스 형태로 반환하도록 설정합니다.
활성화 함수로는 탄젠트(hyperbolic tangent, 'tanh')를 사용합니다.

3. 두 번째 GRU 레이어:

model.add(GRU(50, activation='tanh'))
두 번째 GRU 레이어를 추가합니다.
50개의 뉴런을 가지며, 첫 번째 GRU 레이어의 출력을 입력으로 받습니다.
활성화 함수로는 탄젠트(hyperbolic tangent, 'tanh')를 사용합니다.

4. Flatten 레이어:

model.add(Flatten())
데이터를 Flatten(펼치기)하는 레이어를 추가합니다. 이는 다음 Dense 레이어에 입력하기 위해 2D 데이터를 1D로 변환합니다.

5. Dense 레이어 (출력 레이어):

model.add(Dense(1))
출력 레이어를 추가합니다. 하나의 출력 뉴런을 가지며, 회귀 문제를 다루는 경우 활성화 함수를 지정하지 않거나 'linear'로 지정할 수 있습니다.

6. 모델 요약 출력:

model.summary()를 통해 모델의 구조를 출력합니다.
"""

model.compile(loss='mse', optimizer='adam')
fit_hist = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), shuffle=False)

"""compile 메서드를 사용하여 모델을 컴파일합니다.

loss='mse': 평균 제곱 오차(Mean Squared Error)를 손실 함수로 사용합니다. 회귀 문제에서 주로 사용되는 손실 함수입니다.

optimizer='adam': Adam 옵티마이저를 사용하여 모델을 최적화합니다. Adam은 경사 하강법의 변종 중 하나로, 학습률을 동적으로 조절하여 학습 속도를 최적화합니다.

validation_data는 문제집을 주긴하는데 어떤 문제가 맞았는지 알려 주지 않음.
train으로 훈련하고 test로 검증하는거임

shuffle이 true면 train 학습할 때 순서 랜덤해서 학습함 false하면 순서대로 학습함. 여기선 순서가 중요하니 순서대로 학습해야함.

fit 메서드를 사용하여 모델을 학습합니다.

x_train은 학습 데이터의 입력, y_train은 학습 데이터의 실제 출력(타겟)입니다.

epochs=100은 전체 학습 데이터셋을 100번 반복해서 모델을 학습시킨다는 의미입니다.

validation_data=(x_test, y_test)은 검증 데이터를 사용하여 학습 중에 모델의 성능을 평가하도록 설정합니다.

shuffle=False는 학습 데이터를 에폭마다 섞지 않도록 합니다.

plt.plot(fit_hist.history['loss'][60:], label='loss')
plt.plot(fit_hist.history['val_loss'][60:], label='val_loss')
plt.legend()
plt.show()
"""

plt.plot(fit_hist.history['loss'][60:], label='loss')
plt.plot(fit_hist.history['val_loss'][60:], label='val_loss')
plt.legend()
plt.show()

predict = model.predict(x_test)

plt.plot(y_test[:30], label='actual')
plt.plot(predict[:30], label='predict')
plt.legend()
plt.show()

last_30_data = scaled_data[-30:].reshape(1,30,6)
today_close = model.predict(last_30_data)
print(today_close)

temp = np.array([0, 0, 0, today_close[0][0], 0, 0]).reshape(1,6)
today_close_won = minmax_scaler.inverse_transform(temp)
print(today_close_won[0][3],'원')

