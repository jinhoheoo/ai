# RNN에서 LSTM과 GRU 비교
# RNN(Recurrent Neural Network)은 시계열 데이터나 순차적 데이터를 처리하는 데 사용되는 신경망입니다. RNN은 이전 시간 단계의 출력을 다음 시간 단계의 입력으로 사용하여 순환 구조를 갖고 있습니다.
# 그러나 RNN은 장기 의존성 문제(long-term dependency problem)로 인해 긴 시퀀스를 처리하는 데 어려움을 겪을 수 있습니다.
# 이러한 문제를 해결하기 위해 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)가 개발되었습니다. 이 두 모델은 RNN의 단점을 극복하고 장기 의존성을 더 잘 학습할 수 있도록 설계되었습니다.
#
# 1. LSTM(Long Short-Term Memory)
# 구조: 세 개의 게이트(입력 게이트, 망각 게이트, 출력 게이트)와 셀 상태(cell state)로 구성됩니다. 셀 상태는 장기 의존성을 저장하는 메모리 역할을 합니다.
# 장점: 장기 의존성을 효과적으로 학습할 수 있어 긴 시계열 데이터 처리에 강점이 있습니다.
# 단점: 구조가 복잡하여 학습 속도가 느리고 메모리 사용량이 많습니다.
# 적용 분야: 시계열 예측(주가 예측, 날씨 예측 등), 기계 번역, 음성 인식 등

# 2. GRU(Gated Recurrent Unit)
# 구조: 두 개의 게이트(업데이트 게이트, 리셋 게이트)와 하나의 상태 벡터로 구성됩니다.
# 장점: LSTM보다 구조가 간단하여 학습 속도가 빠르고 메모리 사용량이 적습니다.
# 단점: 장기 의존성 학습 능력이 LSTM보다 약할 수 있습니다.
# 적용 분야: 감성 분석, 문장 생성, 시계열 분류 등

# 4. 선택 기준
# 데이터 길이: 긴 시퀀스는 LSTM, 짧은 시퀀스는 GRU가 적합합니다.
# 연산 자원: 학습 속도와 메모리 사용량이 중요한 경우 GRU, 모델 성능이 더 중요한 경우 LSTM을 선택할 수 있습니다.
# 문제 복잡도: 장기 의존성이 중요한 문제에서는 LSTM이, 빠른 학습과 적은 메모리 사용이 중요한 문제에서는 GRU가 적합합니다.

# -*- coding: utf-8 -*-
# exam18_predict_daily_stock_price.ipynb
#
# Automatically generated by Colaboratory.
#
# Original file is located at
#     https://colab.research.google.com/drive/1jm0SHVyX9PzCtgyV-nnJCKbmq5XZsUaQ
#
# 다음에 어떤 글자가 올지 어떤 시퀀스가 올지를 예측할 때 RNN을 사용함
# 이번 예제는 회귀모델임 즉 예측임 앞에서 cnn이런게 분류모델이였음
# 그리고 시간계열을 예측한 거니 시계열예측임.
#
# 순환 신경망(RNN, Recurrent Neural Network)은 시퀀스 데이터를 처리하고 다음에 올 정보를 예측하는데 효과적입니다. RNN은 특히 시퀀스의 선후 관계가 중요한 경우에 사용됩니다.
#
# 예를 들어, 다음 글자 예측 문제에서 RNN을 사용할 수 있습니다. 주어진 문장에서 이전에 등장한 글자들을 기반으로 다음에 어떤 글자가 올지를 예측하는 것이 목표입니다.
#
# 예시:
#
# 입력: "The quick brown fox"
# 목표: " jumps over the lazy dog."
# 이전의 단어들을 기반으로 "jumps over the lazy dog."라는 문장을 예측하는 것이 RNN이 수행하는 작업입니다. RNN은 순차적으로 입력을 처리하면서 내부에 숨어있는 상태(state)를 업데이트하며, 이 상태는 이전 정보를 기억하고 다음 예측에 영향을 미칩니다.
#
# RNN은 자연어 처리, 음성 인식, 주가 예측 등과 같은 다양한 시퀀스 데이터를 다루는 문제에 적용됩니다. 하지만 RNN은 장기 의존성(Long-Term Dependencies)에 취약한 단점이 있어서
# 이를 보완한 변형인 LSTM(Long Short-Term Memory)나 GRU(Gated Recurrent Unit) 등도 널리 사용됩니다.
# 여기서 장기 의존성이 우리도 시장에가면 게임을 했을 때 앞에 뭐했는지 기억못하는거랑 비슷한거임 앞에 글자는 잊어버리고 뒤쪽 글자인 데이터만 기억해서 그것을 가지고 예측함 5단어 이하정도만 보통 잘됨.
# 순환 신경망(RNN)은 시퀀스 데이터를 처리하고 이전에 발생한 정보를 기억하여 다음에 올 정보를 예측하는 데 사용됩니다.
# 하지만 일반적인 RNN은 긴 시퀀스에서 발생하는 장기 의존성 문제로 인해 성능이 제한될 수 있습니다.
# 이러한 문제를 해결하기 위해 LSTM(Long Short-Term Memory)나 GRU(Gated Recurrent Unit)와 같은 변형된 RNN 구조가 도입되었습니다.
# LSTM은 RNN의 장기 의존성 문제를 해결하기 위해 고안된 구조로, 입력 게이트, 삭제 게이트, 출력 게이트 등을 사용하여 정보를 언제 기억하고 언제 잊을지를 학습합니다.
# 이를 통해 장기 의존성을 뛰어넘을 수 있어서 더 긴 시퀀스 데이터를 처리할 수 있게 되었습니다.
# GRU는 LSTM과 비슷한 목적을 가지고 있지만, 조금 더 간단한 구조로 구성되어 있습니다. LSTM보다 적은 파라미터를 사용하면서도 장기 의존성 문제에 대한 효과적인 대안을 제공합니다.
# 이러한 변형된 RNN 아키텍처들은 자연어 처리, 음성 인식, 주가 예측, 기계 번역 등 다양한 응용 분야에서 기존의 RNN보다 우수한 성능을 보이며, 특
# 히 장기 의존성이 중요한 경우에 효과적으로 사용됩니다.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

raw_data = pd.read_csv('./datasets/samsung.csv')
print(raw_data.head())
raw_data.info()

raw_data['Date'] = pd.to_datetime(raw_data['Date']) #object타입에서 datatime64타입으로 바꿔줌 즉 시간타입으로 바꿔줌
raw_data.info()
print(raw_data.head())

raw_data.set_index('Date', inplace=True)    #Date를 인덱스로 만들어줌
raw_data.info()
print(raw_data.head())

raw_data['Close'].plot()
plt.show()

data_test = raw_data.sort_values('Close')
print(raw_data.head())
print(raw_data.tail())

data_close = raw_data[['Close']]
print(data_close.head())

minmaxscaler = MinMaxScaler()
scaled_data = minmaxscaler.fit_transform(data_close)
print(scaled_data[:])
print(scaled_data.shape)

"""총 6033개의 데이터가 있음."""

sequence_X = []
sequence_Y = []
for i in range(len(scaled_data)-30):
    x = scaled_data[i:i+30]
    y = scaled_data[i+30]
    sequence_X.append(x)
    sequence_Y.append(y)
print(sequence_X[:5])
print(sequence_Y[:5])

sequence_X = np.array(sequence_X)
sequence_Y = np.array(sequence_Y)
print(sequence_X[0])
print(sequence_Y[0])
print(sequence_X.shape)
print(sequence_Y.shape)

x_train, x_test, y_train, y_test = train_test_split(sequence_X,sequence_Y,test_size=0.2)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

model = Sequential()
model.add(LSTM(50, input_shape=(30,1), activation='tanh'))
model.add(Flatten())
model.add(Dense(1)) #값을 예측하는 회귀일 때는 액티베이션안씀 분류 중 단일분류기일 때는 시그모이드, 다중분류기일 때는 소프트맥스 사용함.
model.summary()

model.compile(loss='mse', optimizer='adam')
fit_hist = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), shuffle=False)

plt.plot(fit_hist.history['loss'][60:], label='loss')
plt.plot(fit_hist.history['val_loss'][60:], label='val_loss')
plt.show()

score=model.evaluate(x_test,y_test, verbose=0)
print(score)

"""model.evaluate(x_test, y_test, verbose=0): 모델을 테스트 데이터셋 (x_test, y_test)으로 평가합니다. evaluate 메서드는 손실(loss) 및 다른 지표(metrics)에 대한 평가 결과를 반환합니다. 이 때, x_test는 입력 데이터이고, y_test는 해당 입력에 대한 실제 레이블(타겟)입니다.

verbose=0: 평가 과정에서 출력을 최소화합니다. verbose 매개변수는 출력의 양을 조절하는데 사용되며, 0일 경우 출력을 표시하지 않음을 의미합니다.
"""

pred = model.predict(x_test)

plt.plot(y_test[:30], label='actual')
plt.plot(pred[:30], label='predict')
plt.legend()
plt.show()

last_data_30 = scaled_data[-30:].reshape(1,30,1)
today_close = model.predict(last_data_30)
print(today_close)

today_close_won = minmaxscaler.inverse_transform(today_close)
print('%d 원'%today_close_won[0][0])

"""1. 데이터 전처리:

CSV 파일에서 데이터를 읽어오고, 'Date' 열을 날짜와 시간 형식으로 변환합니다.
'Date' 열을 데이터프레임의 인덱스로 설정합니다.
주식의 종가('Close')를 기준으로 데이터를 정렬하고, 종가 데이터를 따로 저장합니다.
MinMaxScaler를 사용하여 종가 데이터를 정규화합니다.

2. 시퀀스 데이터 생성:

정규화된 종가 데이터를 이용하여, 시퀀스 데이터로 변환합니다. 각 시점에서 이전 30일의 종가를 입력으로 하고, 그 다음 날의 종가를 예측하는 방식으로 시퀀스를 생성합니다.

3. 모델 구축:

LSTM(Long Short-Term Memory)을 사용하여 시계열 예측 모델을 구축합니다. 모델은 입력으로 30일의 종가 데이터를 받고, 출력으로는 그 다음 날의 종가를 반환합니다.

4. 모델 학습:

Train 데이터셋을 사용하여 모델을 학습시킵니다. 손실 함수로는 평균 제곱 오차(Mean Squared Error)를 사용하고, Adam 옵티마이저를 활용합니다.

5. 모델 평가:

Test 데이터셋을 사용하여 모델을 평가합니다. 평가 결과는 주어진 코드에서 score에 저장됩니다.

6. 예측 결과 시각화:

학습된 모델을 사용하여 Test 데이터에 대한 예측을 수행하고, 실제 값과 예측 값을 비교하여 시각화합니다.

7. 미래값 예측:

학습된 모델을 사용하여 마지막 30일의 데이터를 입력으로 하여 다음 날의 종가를 예측합니다. 이 값을 다시 역정규화하여 실제 원화로 변환합니다.


"""