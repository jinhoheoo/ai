# -*- coding: utf-8 -*-
"""exam07_pandas02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pcmIiHSigbFpsyxP_HDCVG1_DFgaJADT

데이터처리방법
-nan 값있으면 값 평균값으로 채워넣거나 별로 쓸모없는 자료면 드랍하거나 하면 됨.
평균 할 때 전체 평균으로 해도 되지만 그룹별로 나눠서 평균해서 정확도 올리는게 좋음.

-0과1사이로 스케일링 하고

-결측값 처리도 해야됨.

-

데이터 처리는 머신러닝 및 통계 분석에서 중요한 단계 중 하나이며, 데이터의 품질을 향상시키고 모델의 성능을 향상시키기 위해 다양한 기법을 사용할 수 있습니다. 여러 가지 일반적인 데이터 처리 방법은 다음과 같습니다:

결측값 처리:
NaN(Not a Number) 값이 있는 경우, 해당 결측값을 적절한 방법으로 처리해야 합니다. 대표적인 방법으로는 평균 값, 중앙 값, 최빈 값으로 대체하거나 해당 행 또는 열을 삭제하는 등이 있습니다.

이상값 처리:
이상값(Outliers)은 데이터 분석에 부정적인 영향을 미칠 수 있습니다. 평균과 표준 편차를 이용한 Z-score나 IQR(Interquartile Range) 등을 활용하여 이상값을 감지하고 처리합니다.

스케일링:
변수 간의 값의 범위가 크게 다를 경우, 스케일링을 통해 데이터의 크기를 조절할 수 있습니다. 대표적으로 Min-Max 스케일링이나 표준화(Standardization)를 사용합니다.

원-핫 인코딩 (One-Hot Encoding):
범주형 변수를 머신러닝 모델에 적용하기 위해 더미 변수로 변환합니다. pd.get_dummies() 함수 등을 사용하여 수행할 수 있습니다.

그룹별 통계량 계산:
데이터를 특정 기준에 따라 그룹화하고, 각 그룹에 대한 통계량(평균, 중앙 값, 표준 편차 등)을 계산하여 변수 간 관계를 더 잘 이해할 수 있습니다.

정규화 (Normalization):
데이터의 분포를 정규화하여 모델이 더 잘 학습할 수 있도록 도와줍니다. 주로 0과 1 사이의 값으로 스케일을 조정하는 방법을 사용합니다.

변수 선택 및 차원 축소:
불필요한 변수를 제거하거나 주성분 분석(PCA) 등을 사용하여 데이터의 차원을 축소할 수 있습니다.

시계열 데이터 처리:
시계열 데이터의 경우, 시간에 따른 패턴을 이해하고 추세를 제거하는 등의 작업이 필요할 수 있습니다.

이러한 데이터 처리 단계는 데이터의 특성에 따라 다르게 적용될 수 있습니다. 데이터의 특성을 잘 이해하고, 모델의 학습을 개선하기 위한 적절한 전처리 방법을 선택하는 것이 중요합니다.

1. 데이터 불러오기: 데이터를 수집하여 적절한 형태로 불러옵니다. 주로 사용되는 도구로는 Pandas, NumPy 등이 있습니다.

import pandas as pd

- CSV 파일로부터 데이터 불러오기

data = pd.read_csv('your_dataset.csv')

----------------
2. 데이터 이해 및 탐색: 데이터의 구조, 변수 유형, 기본 통계량을 확인하여 데이터를 이해합니다.

- 데이터의 기본 정보 확인

print(data.info())

- 데이터의 통계량 확인

print(data.describe())

---------------------------

3. 결측값 및 이상값 처리: 결측값과 이상값을 처리하여 데이터의 무결성을 유지합니다.

- 결측값 처리 (예: 평균값으로 대체)

data['column_name'].fillna(data['column_name'].mean(), inplace=True)

- 이상값 처리 (예: Z-score 기반 제거)

from scipy import stats
z_scores = stats.zscore(data['column_name'])
data = data[(z_scores < 3) & (z_scores > -3)]

------------------

4. 데이터 변환: 범주형 변수를 더미 변수로 변환하거나, 연속형 변수를 스케일링하는 등의 변환을 수행합니다.

- 범주형 변수를 더미 변수로 변환

data = pd.get_dummies(data, columns=['categorical_column'])

- 연속형 변수 스케일링 (예: Min-Max 스케일링)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data['numeric_column'] = scaler.fit_transform(data[['numeric_column']])

범주형 변수(Categorical Variable):
범주형 변수는 명목형 변수와 순서형 변수로 나뉩니다.

명목형 변수(Nominal Variable):
명목형 변수는 각 범주 간에 순서가 없는 경우를 의미합니다. 예를 들어, 성별(Gender)이나 도시(City) 등이 명목형 변수입니다.

순서형 변수(Ordinal Variable):
순서형 변수는 범주 간에 순서가 있는 경우를 의미합니다. 예를 들어, 학점(A, B, C)이나 선호도(매우 좋음, 좋음, 보통, 나쁨, 매우 나쁨) 등이 순서형 변수입니다.

연속형 변수(Continuous Variable):
연속형 변수는 연속적인 값을 가지는 변수로, 측정이나 계량이 가능한 변수를 나타냅니다. 예를 들어, 키, 몸무게, 온도, 속도 등이 연속형 변수입니다. 연속형 변수는 무한히 많은 값 중 어느 값을 가질 수 있으며, 연속적인 범위를 가지고 있습니다.

명목형 변수:
성별: 남성, 여성
동물의 종: 개, 고양이, 새

순서형 변수:
학점: A, B, C
선호도: 매우 좋음, 좋음, 보통, 나쁨, 매우 나쁨

연속형 변수:
키: 175.2 cm
몸무게: 68.5 kg
온도: 25.3°C

------------------
5.변수 선택 및 차원 축소: 불필요한 변수를 제거하거나 주성분 분석 (PCA) 등을 사용하여 차원을 축소합니다.

- 불필요한 변수 제거

data.drop(['unnecessary_column'], axis=1, inplace=True)

- 주성분 분석 (PCA)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(data[['feature1', 'feature2']])

6.데이터 저장: 전처리가 완료된 데이터를 적절한 형식으로 저장합니다.

- CSV 파일로 저장

data.to_csv('preprocessed_data.csv', index=False)
"""

import pandas as pd

# 기존 데이터프레임 생성 (예시)
data = {'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 35]}
df = pd.DataFrame(data)

# 새로운 인덱스 추가
df['NewIndex'] = [100, 200, 300]

# 또는 기존 인덱스를 변경할 수도 있습니다.
df.index = ['A', 'B', 'C']

print(df)

# 기존 데이터프레임 생성 (예시)
data = {'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 35]}
df = pd.DataFrame(data)

# 새로운 열 추가
df['City'] = ['New York', 'San Francisco', 'Los Angeles']

# 또는 다른 방법으로 열 추가
df.insert(loc=2, column='Occupation', value=['Engineer', 'Doctor', 'Artist'])

print(df)

pd.set_option('display.max_columns',15)  #숫자 6을 적으면 아래 칸에 names에 9가지의 콜럼명들이 있는데 이 중 6가지만 보여주고 중간거는 ...으로 표현한다는거임 15는 9보다 크니까 전부보여줌
pd.set_option('display.max_rows',30)  #뒤에 숫자가 전체 행의 수보다 많이하면 ...없이 전부다 보여줌.

df = pd.read_csv('./datasets/auto-mpg.csv', names=['mpg','cylinders','displacement','horsepower','weight','acceleration','model year','origin','name'])
df

df.head(10) #앞의 10개 보여줌 아무것도 안적으면 5개만 보여줌

df.tail() #젤 뒤의 5개만 보여줌

df.info() #자료가 몇개고 자료에 맟춰 인덱스가 몇개고 데이터타입이 뭐고 등등알려줌

"""horsepower,name 이 object인걸로 봐선 ?나 이런게 있어서 해결해야됨 숫자가 아닌게 있어서그럼
nan 이런건 데이터타입이 float형임
"""

df.dtypes #데이터타입을 알려줌

df['mpg'].dtypes #특정 데이터의 타입을 알려줌

df.describe().T #기술통계량을 보여줌 .T를 붙여서 열과 행을 바꿔줌

"""7가지 밖에 안나온이유는 horsepower이랑 names의 자료형이 object라서 그럼"""

df.describe(include='all')

"""count:
각 열에 대한 비어 있지 않은 값의 개수를 나타냅니다.

unique:
각 열에서 고유한 값의 개수를 나타냅니다.
"NaN"은 결측값(Not a Number)이나 비어 있는 값의 개수를 나타냅니다.

top:
각 열에서 가장 많이 나타나는 값(최빈값)을 나타냅니다.

freq:
가장 많이 나타나는 값의 빈도(빈도수)를 나타냅니다.

"NaN"이 나타나는 열은 주로 수치형 데이터 타입인 "int" 또는 "float"일 것으로 예상됩니다. "object" 타입은 주로 문자열이나 혼합된 데이터 타입을 나타내므로, "NaN"이 표시되지 않을 수 있습니다.

"NaN"이 나타나는 열에 대한 처리를 위해서는 결측값을 적절히 처리해야 합니다. 일반적으로는 결측값을 해당 열의 평균값, 중앙값, 또는 다른 특정 값으로 대체하는 방법을 사용합니다. Pandas에서는 fillna() 메서드를 사용하여 결측값을 대체할 수 있습니다.

예를 들어, "horsepower" 열의 결측값을 해당 열의 평균값으로 대체하는 코드는 다음과 같을 수 있습니다:
df['horsepower'].fillna(df['horsepower'].mean(), inplace=True)
"""

unique_value = df['name'].value_counts()
print(type(unique_value))
unique_value

df['model year'].value_counts()

df.mean() #평균내줌

df.mpg.mean()

df.corr() #상관계수를 출력해줌

"""상관계수는 두 변수 간의 관계가 얼마나 강한지를 나타내는 지표입니다.

1에 가까우면 양의 상관관계: 하나가 증가하면 다른 하나도 증가하는 경향이 있습니다.

-1에 가까우면 음의 상관관계: 하나가 증가하면 다른 하나는 감소하는 경향이 있습니다.

0에 가까우면 상관관계가 약함: 두 변수 간에 선형적인 관계가 거의 없습니다.

예를 들어, 자동차의 연비("mpg")와 무게("weight") 간의 상관관계가 -0.8이라면, 무게가 증가할수록 연비는 감소하는 경향이 강하다는 것을 의미합니다. 상관계수는 -1에서 1 사이의 값을 가지며, 값이 크면 클수록 두 변수 간의 관계가 강하다고 볼 수 있습니다.

판다스에서는 df.corr() 메서드를 사용하여 데이터프레임 내의 모든 변수 간의 상관계수를 한 번에 확인할 수 있습니다. 상관계수는 데이터의 특성을 더 잘 이해하고 관련성을 파악하는데 도움을 줍니다.
"""

#연비 알아보기 힘들어서 mpg를 kpl단위로 바꾸려함
mpg_to_kpl = 0.425144
df['kpl'] = df['mpg'] * mpg_to_kpl
df.head(50)

df['kpl']=df['kpl'].round(2)  #두 자리까지만 반올림
df

df['horsepower'].unique()

"""문자열이 float보다 크기때문에 string으로 맞춰줌 ?때문에"""

import numpy as np

df['horsepower'].replace('?', np.nan, inplace=True) #?를 nan으로 바꿈
df.dropna(subset=['horsepower'], axis=0, inplace=True)  #nan 있는 라인을 드롭함 그리고 행을 삭제하니 0임
df['horsepower'] = df['horsepower'].astype('float64')
df.info()

df['origin'].unique()

df['origin'].replace({1:'USA',2:'EU',3:'JP'}, inplace=True)
print(df['origin'].unique)
print(df['origin'].head(30))
print(df['origin'].value_counts)

df['origin'] = df['origin'].astype('category')  #자료형을 카테고리타입으로 바꿈
print(df['origin'].dtypes)
print(df['origin'])

"""척도 4가지

명목척도

서열척도는 비교만 될뿐 계산이 안되는 그런 척도임 즉 1등이90점이고 2등이 85점인데 3등은 몇점일까 그건 모름 그래서 서열만 알 수 있음

등간척도는 구간을 나눠서 구간마다의 크기를 정해서 파악하는 척도임 즉 100m를 10개의 구간으로 나눠서 한번에 3칸을 뛰면 30m갔구나 알수 있고 이런거임
그래서 등간척도는 int처럼 정수느낌임

비율척도는 float같이 1.1이런 느낌임

명목척도,서열척도는 계산이 안되고 등간척도와 비율척도는 계산을 할 수 있음.

명목척도 (Nominal Scale):
데이터 간에 구분만 가능하고 순서나 계산이 불가능한 수준입니다.
예: 성별, 인종, 도시 이름 등

서열척도 (Ordinal Scale):
데이터 간에 순서를 부여할 수 있지만 간격이나 비율은 측정할 수 없는 수준입니다.
예: 성적 순위, 만족도 순위 등

등간척도 (Interval Scale):
데이터 간의 순서와 함께 간격도 측정 가능한 수준입니다. 하지만 비율은 측정할 수 없습니다.
예: 온도 (0도가 정해진 시작점이 아니며, 10도와 20도 간의 간격은 동일하지만 20도와 30도 간의 간격은 동일하지 않음)

비율척도 (Ratio Scale):
데이터 간의 순서, 간격, 비율을 모두 측정할 수 있는 가장 높은 수준입니다.
예: 길이, 무게, 시간, 온도 (0도가 절대적인 제로인 경우, 10도와 20도 간의 간격과 20도와 30도 간의 간격이 동일함)
"""

df['origin'] = df['origin'].astype('str')  #자료형을 str으로 바꾸면 object가 됨.
print(df['origin'].dtypes)
print(df['origin'])

count, bin_dividers = np.histogram(df['horsepower'], bins=3)
print(count)
print(bin_dividers)

"""bins로 나누고싶은 구간개수 정한거고
count에 각 구간에 포함된 개수가 들어가고
bin_di 여기에 시작과 끝값을 기준으로 구간의 개수에 맞춰 경계를 나눈 값임

np.histogram() 함수는 주어진 데이터를 히스토그램으로 변환하는데 사용됩니다. 여기서 df['horsepower']는 히스토그램을 그릴 데이터이며, bins=3는 히스토그램의 구간(bin)의 수를 나타냅니다.

np.histogram() 함수는 히스토그램의 빈도(count)와 각 구간의 경계(bin_dividers)를 반환합니다.

히스토그램은 데이터의 분포를 시각적으로 나타내는 그래픽 표현 방법 중 하나입니다. 주로 연속적인 데이터의 빈도를 구간으로 나눠 표현합니다. 데이터가 특정 구간에 속하는 빈도를 막대로 나타내어 전반적인 데이터의 분포를 파악할 수 있습니다.

히스토그램을 만들기 위해서는 데이터를 일정한 간격(구간)으로 나누고, 각 구간에 속하는 데이터의 빈도를 막대로 표현합니다. 이를 통해 데이터의 분포, 중심 경향성, 퍼짐 정도 등을 시각적으로 이해할 수 있습니다.

간단한 예시로, 자동차의 마력(horsepower) 데이터를 사용하여 히스토그램을 만들 수 있습니다. 만약 마력이 100 미만, 100 이상 200 미만, 200 이상의 세 구간으로 나눈다면, 각 구간에 해당하는 자동차의 개수를 막대로 표현한 것이 히스토그램이 됩니다.


"""

bin_names = ['저출력','보통출력','고출력']
df['hp_bin'] = pd.cut(x=df['horsepower'], bins=bin_dividers, labels=bin_names, include_lowest=True)
df[['horsepower','hp_bin']].head(30)

"""pd.cut을 이용해서 카테고리별로 나눌수 있음.
hp_bin을 정의해주려하고 어떤거 쓸지랑 어떤 경계값인지랑 label 저기에 각 구역의 이름 정해주고 첫구간의 시작값을 포함할지한거임.

"horsepower" 열의 값을 pd.cut() 함수를 사용하여 이산화하고, 새로운 "hp_bin" 열을 생성하는 부분입니다. 각 매개변수의 역할은 다음과 같습니다:

x=df['horsepower']: 이산화할 대상이 되는 데이터. 여기서는 "horsepower" 열의 값이 사용됩니다.
bins=bin_dividers: 나눌 구간의 경계값. 앞서 계산한 히스토그램의 구간 경계값인 bin_dividers가 사용됩니다.
labels=bin_names: 각 구간에 대응하는 레이블. 여기서는 "bin_names"으로 지정한 레이블이 사용됩니다.
include_lowest=True: 첫 번째 구간의 시작값을 포함할지 여부. 여기서는 True로 지정하여 첫 번째 구간의 시작값을 포함시켰습니다.
"""

df.info()

df1=df[['horsepower','hp_bin','origin']]
df1

df2 = pd.get_dummies(df1)

"""pd.get_dummies() 함수는 데이터프레임의 범주형(categorical) 변수를 더미(dummy) 변수로 변환하는데 사용됩니다. 범주형 변수는 일반적으로 문자열 레이블이나 범주형 숫자로 표현되는 변수를 말하며, 이러한 변수를 더미 변수로 변환하면 머신러닝 모델에 적용하기 용이해집니다. 더미 변수는 이러한 범주형 변수를 0 또는 1의 값으로 구성된 이진 변수로 변환하는 방법 중 하나입니다."""

df2

df = pd.DataFrame({'c1':['a','a','b','a','b'],'c2':[1,1,1,2,2],'c3':[1,1,2,2,2]})
df

df_dup = df.duplicated()  #duplicated()에서 완전 다 중복이면 true 아니면 false
df_dup

df_dup = df['c2'].duplicated()  #c2만가지고 중복을 체크하는거임 예를들어 이름은 중복 될수 있는데 학번은 중복되면 안되니까 그런경우
df_dup

df2 = df.drop_duplicates()  #중복을 제거
df2

df2.info()

df2.iloc[1] #iloc 니까 우리가 지정한 인덱스 이름이 아닌 자동으로 설정되는 인덱스 번호라서 1하면 위의 2번을 의미함

df2.loc[2] #loc니까 우리가 정한 이름인 인덱스임 그래서 위와 같음.

df2.reset_index(drop=True, inplace=True)  #인덱스 이름을 리셋시킴
df2